{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6b3074-5cc5-4870-84ac-c2090cdd79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification, AdamW\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d8d802-3ccb-48bd-bc64-614461b47e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f613a62eaa46a3ac52b589e077774e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380afb319faf484cb063fc9e04fc6645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107b3ed6f0984cda9ab6130664841496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be21d29f996467dbddd6798faf35480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392389ab07d14394a9fbce40d68eb1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-large-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6041461f-779d-4212-b9df-19678819c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a9e8aca-1a5f-49f1-8ffe-f2d55ef0b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenizer(list(train_df['text']), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fe0e794-159f-4ddb-95f9-aed133235b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_df['target'].values  \n",
    "))\n",
    "train_dataset = train_dataset.shuffle(len(train_df)).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "323c2589-64d8-4d8a-a26d-e5196ed3a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "num_train_steps = len(train_dataset) * 3  #keep n mind it takes time :)\n",
    "optimizer, _ = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9637e29-9abb-4568-af27-db9f7cfb2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = ['accuracy']\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3ff7233-1c93-42cf-99ce-8f6aea8d02f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x156d2bf60> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x156d2bf60> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "476/476 [==============================] - 9801s 20s/step - loss: 0.4278 - accuracy: 0.8166\n",
      "Epoch 2/3\n",
      "476/476 [==============================] - 69718s 147s/step - loss: 0.3146 - accuracy: 0.8824\n",
      "Epoch 3/3\n",
      "476/476 [==============================] - 101234s 213s/step - loss: 0.2229 - accuracy: 0.9199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x15d7a6300>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=3) #you can adjust this but it won't be so accurate anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77e1d615-7adb-4024-bcee-286e7f8aceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 632s 3s/step\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "test_encodings = tokenizer(list(test_df['text']), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(dict(test_encodings))\n",
    "test_dataset = test_dataset.batch(16)\n",
    "\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "predicted_labels = tf.argmax(predictions.logits, axis=-1).numpy()\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],  # Zakładamy, że kolumna 'id' istnieje\n",
    "    \"target\": predicted_labels\n",
    "})\n",
    "submission.to_csv(\"submission_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4a140-86c5-4203-9f31-5a583975e0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f1bfc-9f81-4661-b7ae-94f07d138e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32c3aa-c56c-4cbb-856d-d169e7f0209a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b86351-c940-465b-8636-3401b9b654cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
